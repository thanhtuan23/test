{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-28T11:37:59.744925Z",
     "iopub.status.busy": "2021-12-28T11:37:59.744505Z",
     "iopub.status.idle": "2021-12-28T11:37:59.750737Z",
     "shell.execute_reply": "2021-12-28T11:37:59.750011Z",
     "shell.execute_reply.started": "2021-12-28T11:37:59.74486Z"
    }
   },
   "source": [
    "# Network Intrusion Detection Using Machine Learning/Deep Learning\n",
    "This notebook involves the making of machine learning & deep learning models to classify the given data of obtained as a network intrusion into differen classes (malignant or benign). Given a sample point, the objective of machine learning model will be to classify that whether the intrusion made is  **Benign** or is a **BruteForce** (either FTP or SSH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "First, we will import libraries that we need to start our workflow. The libraries we are using are:\n",
    "* NumPy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Scikit-learn\n",
    "* Keras\n",
    "* TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re, time, math, tqdm, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import plotly.offline as pyo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:05:56.696445Z",
     "iopub.status.busy": "2022-01-07T20:05:56.69617Z",
     "iopub.status.idle": "2022-01-07T20:05:56.724869Z",
     "shell.execute_reply": "2022-01-07T20:05:56.724192Z",
     "shell.execute_reply.started": "2022-01-07T20:05:56.696409Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the available data\n",
    "data_folder = 'dataset'\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    print(f\"Error: Folder {data_folder} does not exist!\")\n",
    "elif not os.listdir(data_folder):\n",
    "    print(f\"Error: Folder {data_folder} is empty!\")\n",
    "else:\n",
    "    print(\"Folder exists and has files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of data available to deal with in this notebook. We will perform analysis, preprocessing and modeling on one of the datasets and will conclude the results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  2 11:04:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 571.96                 Driver Version: 571.96         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P0             15W /   65W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "First step is to load the available data into our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 files in dataset\n",
      "CPU times: total: 4.8 s\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the data into memory\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(data_folder):\n",
    "    for file in files:\n",
    "        all_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(all_files)} files in {data_folder}\")\n",
    "\n",
    "# load the data into memory\n",
    "network_data = pd.read_csv(all_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)\n",
    "For making a proper undertanding of dataset we are using, we will perform a bief EDA (Exploratory Data Analysis). The EDA is sub-divided into:\n",
    "* Data Visuals\n",
    "* Data Understanding\n",
    "* Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:07:03.471572Z",
     "iopub.status.busy": "2022-01-07T20:07:03.4709Z",
     "iopub.status.idle": "2022-01-07T20:07:03.477017Z",
     "shell.execute_reply": "2022-01-07T20:07:03.476251Z",
     "shell.execute_reply.started": "2022-01-07T20:07:03.471535Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the shape of data\n",
    "network_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:07:16.858135Z",
     "iopub.status.busy": "2022-01-07T20:07:16.857866Z",
     "iopub.status.idle": "2022-01-07T20:07:16.863503Z",
     "shell.execute_reply": "2022-01-07T20:07:16.862837Z",
     "shell.execute_reply.started": "2022-01-07T20:07:16.858107Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of rows and columns\n",
    "print('Number of Rows (Samples): %s' % str((network_data.shape[0])))\n",
    "print('Number of Columns (Features): %s' % str((network_data.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of **1 million+** samples and **80** features in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:07:19.369972Z",
     "iopub.status.busy": "2022-01-07T20:07:19.369406Z",
     "iopub.status.idle": "2022-01-07T20:07:19.403497Z",
     "shell.execute_reply": "2022-01-07T20:07:19.402731Z",
     "shell.execute_reply.started": "2022-01-07T20:07:19.369932Z"
    }
   },
   "outputs": [],
   "source": [
    "network_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:07:24.477479Z",
     "iopub.status.busy": "2022-01-07T20:07:24.476803Z",
     "iopub.status.idle": "2022-01-07T20:07:24.484469Z",
     "shell.execute_reply": "2022-01-07T20:07:24.483716Z",
     "shell.execute_reply.started": "2022-01-07T20:07:24.477443Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the columns in data\n",
    "network_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:07:29.426936Z",
     "iopub.status.busy": "2022-01-07T20:07:29.426412Z",
     "iopub.status.idle": "2022-01-07T20:07:29.433239Z",
     "shell.execute_reply": "2022-01-07T20:07:29.431364Z",
     "shell.execute_reply.started": "2022-01-07T20:07:29.426899Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of columns\n",
    "print('Total columns in our data: %s' % str(len(network_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is huge. We have a total of **80** columns in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:07:39.61417Z",
     "iopub.status.busy": "2022-01-07T20:07:39.613901Z",
     "iopub.status.idle": "2022-01-07T20:07:39.972932Z",
     "shell.execute_reply": "2022-01-07T20:07:39.971857Z",
     "shell.execute_reply.started": "2022-01-07T20:07:39.614142Z"
    }
   },
   "outputs": [],
   "source": [
    "network_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following information tells us that:\n",
    "* We have a huge amount of data, containing **1 million+** entries (samples)\n",
    "* There are a total of **80** columns belinging to each sample\n",
    "* There are missing values in our data, which need to be filled or dropped for proper modelling\n",
    "* The memory consumption of data is **700 MB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:07:56.049858Z",
     "iopub.status.busy": "2022-01-07T20:07:56.04955Z",
     "iopub.status.idle": "2022-01-07T20:07:56.207495Z",
     "shell.execute_reply": "2022-01-07T20:07:56.206793Z",
     "shell.execute_reply.started": "2022-01-07T20:07:56.049824Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of values for labels\n",
    "network_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the network intrusions in our data are benign, as output from above code cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations\n",
    "After getting some useful information about our data, we now make visuals of our data to see how the trend in our data goes like. The visuals include bar plots, distribution plots, scatter plots, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:08:48.427197Z",
     "iopub.status.busy": "2022-01-07T20:08:48.426705Z",
     "iopub.status.idle": "2022-01-07T20:08:49.888508Z",
     "shell.execute_reply": "2022-01-07T20:08:49.887662Z",
     "shell.execute_reply.started": "2022-01-07T20:08:48.427163Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a plot number of labels\n",
    "sns.set(rc={'figure.figsize':(12, 6)})\n",
    "plt.xlabel('Attack Type')\n",
    "sns.set_theme()\n",
    "ax = sns.countplot(x='Label', data=network_data)\n",
    "ax.set(xlabel='Attack Type', ylabel='Number of Attacks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:11:02.239832Z",
     "iopub.status.busy": "2022-01-07T20:11:02.239522Z",
     "iopub.status.idle": "2022-01-07T20:11:03.069922Z",
     "shell.execute_reply": "2022-01-07T20:11:03.069261Z",
     "shell.execute_reply.started": "2022-01-07T20:11:02.239771Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a scatterplot\n",
    "pyo.init_notebook_mode()\n",
    "fig = px.scatter(x = network_data[\"Bwd Pkts/s\"][:100000], \n",
    "                 y=network_data[\"Fwd Seg Size Min\"][:100000])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:11:15.355417Z",
     "iopub.status.busy": "2022-01-07T20:11:15.354682Z",
     "iopub.status.idle": "2022-01-07T20:11:19.691348Z",
     "shell.execute_reply": "2022-01-07T20:11:19.690672Z",
     "shell.execute_reply.started": "2022-01-07T20:11:15.355381Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.set(rc={'figure.figsize':(12, 6)})\n",
    "sns.scatterplot(x=network_data['Bwd Pkts/s'][:50000], y=network_data['Fwd Seg Size Min'][:50000], \n",
    "                hue='Label', data=network_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs, we came to know that:\n",
    "* Most of the attacks made by intruders are malignant (almost 700k)\n",
    "* **FTP-BruteFore** and **SSH-BruteForce** type attacks are less in numbers (less than 200k)\n",
    "* Most of the intruders try to make a malignant attack on network systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:11:42.206882Z",
     "iopub.status.busy": "2022-01-07T20:11:42.206594Z",
     "iopub.status.idle": "2022-01-07T20:11:42.212613Z",
     "shell.execute_reply": "2022-01-07T20:11:42.211579Z",
     "shell.execute_reply.started": "2022-01-07T20:11:42.206851Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the dtype of timestamp column\n",
    "(network_data['Timestamp'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Data preprocessing plays an important part in the process of data science, since data may not be fully clean and can contain missing or null values. In this step, we are undergoing some preprocessing steps that will help us if there is any null or missing value in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:11:44.396989Z",
     "iopub.status.busy": "2022-01-07T20:11:44.395237Z",
     "iopub.status.idle": "2022-01-07T20:11:44.722747Z",
     "shell.execute_reply": "2022-01-07T20:11:44.721988Z",
     "shell.execute_reply.started": "2022-01-07T20:11:44.396936Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for some null or missing values in our dataset\n",
    "network_data.isna().sum().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features in the data have no null or missing values, except one feature that contains **2277** missing values. We need to remove this column from our data, so that our data may get cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:13:09.512301Z",
     "iopub.status.busy": "2022-01-07T20:13:09.512015Z",
     "iopub.status.idle": "2022-01-07T20:13:10.3769Z",
     "shell.execute_reply": "2022-01-07T20:13:10.376043Z",
     "shell.execute_reply.started": "2022-01-07T20:13:09.512271Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop null or missing columns\n",
    "cleaned_data = network_data.dropna()\n",
    "cleaned_data.isna().sum().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the missing valued column in our data, we have now no feature that contains any missing or null value. Data is cleaned now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "The Label feature in the data contains 3 labels as **Benign**, **BruteForceFTP** and **BruteForceSSH**. All these are in string format. For our neural network, we need to convert them into numbers so that our NN may understand their representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:21:10.739835Z",
     "iopub.status.busy": "2022-01-07T20:21:10.739559Z",
     "iopub.status.idle": "2022-01-07T20:21:10.783896Z",
     "shell.execute_reply": "2022-01-07T20:21:10.783141Z",
     "shell.execute_reply.started": "2022-01-07T20:21:10.739805Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode the column labels\n",
    "label_encoder = LabelEncoder()\n",
    "cleaned_data['Label']= label_encoder.fit_transform(cleaned_data['Label'])\n",
    "cleaned_data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:21:13.390953Z",
     "iopub.status.busy": "2022-01-07T20:21:13.390389Z",
     "iopub.status.idle": "2022-01-07T20:21:13.402448Z",
     "shell.execute_reply": "2022-01-07T20:21:13.401675Z",
     "shell.execute_reply.started": "2022-01-07T20:21:13.390914Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for encoded labels\n",
    "cleaned_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping the data for CNN\n",
    "For applying a convolutional neural network on our data, we will have to follow following steps:\n",
    "* Seperate the data of each of the labels\n",
    "* Create a numerical matrix representation of labels\n",
    "* Apply resampling on data so that can make the distribution equal for all labels\n",
    "* Create X (predictor) and Y (target) variables\n",
    "* Split the data into train and test sets\n",
    "* Make data multi-dimensional for CNN\n",
    "* Apply CNN on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:26:22.315581Z",
     "iopub.status.busy": "2022-01-07T20:26:22.315321Z",
     "iopub.status.idle": "2022-01-07T20:26:23.213443Z",
     "shell.execute_reply": "2022-01-07T20:26:23.212713Z",
     "shell.execute_reply.started": "2022-01-07T20:26:22.315547Z"
    }
   },
   "outputs": [],
   "source": [
    "# make 3 seperate datasets for 3 feature labels\n",
    "data_1 = cleaned_data[cleaned_data['Label'] == 0]\n",
    "data_2 = cleaned_data[cleaned_data['Label'] == 1]\n",
    "data_3 = cleaned_data[cleaned_data['Label'] == 2]\n",
    "\n",
    "# make benign feature\n",
    "y_1 = np.zeros(data_1.shape[0])\n",
    "y_benign = pd.DataFrame(y_1)\n",
    "\n",
    "# make bruteforce feature\n",
    "y_2 = np.ones(data_2.shape[0])\n",
    "y_bf = pd.DataFrame(y_2)\n",
    "\n",
    "# make bruteforceSSH feature\n",
    "y_3 = np.full(data_3.shape[0], 2)\n",
    "y_ssh = pd.DataFrame(y_3)\n",
    "\n",
    "# merging the original dataframe\n",
    "X = pd.concat([data_1, data_2, data_3], sort=True)\n",
    "y = pd.concat([y_benign, y_bf, y_ssh], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:26:35.204534Z",
     "iopub.status.busy": "2022-01-07T20:26:35.204289Z",
     "iopub.status.idle": "2022-01-07T20:26:35.211638Z",
     "shell.execute_reply": "2022-01-07T20:26:35.210756Z",
     "shell.execute_reply.started": "2022-01-07T20:26:35.204507Z"
    }
   },
   "outputs": [],
   "source": [
    "y_1, y_2, y_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:27:07.797727Z",
     "iopub.status.busy": "2022-01-07T20:27:07.796988Z",
     "iopub.status.idle": "2022-01-07T20:27:07.802664Z",
     "shell.execute_reply": "2022-01-07T20:27:07.801971Z",
     "shell.execute_reply.started": "2022-01-07T20:27:07.797691Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:27:10.821792Z",
     "iopub.status.busy": "2022-01-07T20:27:10.82152Z",
     "iopub.status.idle": "2022-01-07T20:27:11.043195Z",
     "shell.execute_reply": "2022-01-07T20:27:11.042493Z",
     "shell.execute_reply.started": "2022-01-07T20:27:10.821751Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking if there are some null values in data\n",
    "X.isnull().sum().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of above cell shows that there are no null values in our data, and the data can now be used for model fitting. We have two types of datasets, normal and abnormal, and they'll be used for model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Argumentation\n",
    "Ti avoid biasing in data, we need to use data argumentation on it so that we can remove bias from data and make equal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:27:52.833869Z",
     "iopub.status.busy": "2022-01-07T20:27:52.83343Z",
     "iopub.status.idle": "2022-01-07T20:27:52.937886Z",
     "shell.execute_reply": "2022-01-07T20:27:52.937126Z",
     "shell.execute_reply.started": "2022-01-07T20:27:52.833827Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "data_1_resample = resample(data_1, n_samples=20000, \n",
    "                           random_state=123, replace=True)\n",
    "data_2_resample = resample(data_2, n_samples=20000, \n",
    "                           random_state=123, replace=True)\n",
    "data_3_resample = resample(data_3, n_samples=20000, \n",
    "                           random_state=123, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:27:55.700642Z",
     "iopub.status.busy": "2022-01-07T20:27:55.700365Z",
     "iopub.status.idle": "2022-01-07T20:27:55.74531Z",
     "shell.execute_reply": "2022-01-07T20:27:55.744481Z",
     "shell.execute_reply.started": "2022-01-07T20:27:55.700612Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.concat([data_1_resample, data_2_resample, data_3_resample])\n",
    "train_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:34:02.20557Z",
     "iopub.status.busy": "2022-01-07T20:34:02.205312Z",
     "iopub.status.idle": "2022-01-07T20:34:02.301319Z",
     "shell.execute_reply": "2022-01-07T20:34:02.300546Z",
     "shell.execute_reply.started": "2022-01-07T20:34:02.205542Z"
    }
   },
   "outputs": [],
   "source": [
    "# viewing the distribution of intrusion attacks in our dataset \n",
    "plt.figure(figsize=(10, 8))\n",
    "circle = plt.Circle((0, 0), 0.7, color='white')\n",
    "plt.title('Intrusion Attack Type Distribution')\n",
    "plt.pie(train_dataset['Label'].value_counts(), labels=['Benign', 'BF', 'BF-SSH'], colors=['blue', 'magenta', 'cyan'])\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making X & Y Variables (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:36:23.776461Z",
     "iopub.status.busy": "2022-01-07T20:36:23.776209Z",
     "iopub.status.idle": "2022-01-07T20:36:23.79435Z",
     "shell.execute_reply": "2022-01-07T20:36:23.793681Z",
     "shell.execute_reply.started": "2022-01-07T20:36:23.776434Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = train_dataset.sample(frac=0.1)\n",
    "target_train = train_dataset['Label']\n",
    "target_test = test_dataset['Label']\n",
    "target_train.unique(), target_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:36:29.300285Z",
     "iopub.status.busy": "2022-01-07T20:36:29.299722Z",
     "iopub.status.idle": "2022-01-07T20:36:29.306904Z",
     "shell.execute_reply": "2022-01-07T20:36:29.305643Z",
     "shell.execute_reply.started": "2022-01-07T20:36:29.300246Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(target_train, num_classes=3)\n",
    "y_test = to_categorical(target_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splicing\n",
    "This stage involves the data split into train & test sets. The training data will be used for training our model, and the testing data will be used to check the performance of model on unseen dataset. We're using a split of **80-20**, i.e., **80%** data to be used for training & **20%** to be used for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:36:32.804456Z",
     "iopub.status.busy": "2022-01-07T20:36:32.804195Z",
     "iopub.status.idle": "2022-01-07T20:36:32.820349Z",
     "shell.execute_reply": "2022-01-07T20:36:32.81964Z",
     "shell.execute_reply.started": "2022-01-07T20:36:32.804427Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts/s\",\"Flow Pkts/s\", \"Label\"], axis=1)\n",
    "test_dataset = test_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts/s\",\"Flow Pkts/s\", \"Label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:36:38.769626Z",
     "iopub.status.busy": "2022-01-07T20:36:38.769157Z",
     "iopub.status.idle": "2022-01-07T20:36:38.801563Z",
     "shell.execute_reply": "2022-01-07T20:36:38.800846Z",
     "shell.execute_reply.started": "2022-01-07T20:36:38.76959Z"
    }
   },
   "outputs": [],
   "source": [
    "# making train & test splits\n",
    "X_train = train_dataset.iloc[:, :-1].values\n",
    "X_test = test_dataset.iloc[:, :-1].values\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:36:43.755736Z",
     "iopub.status.busy": "2022-01-07T20:36:43.755367Z",
     "iopub.status.idle": "2022-01-07T20:36:43.76182Z",
     "shell.execute_reply": "2022-01-07T20:36:43.761102Z",
     "shell.execute_reply.started": "2022-01-07T20:36:43.755688Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:37:01.07497Z",
     "iopub.status.busy": "2022-01-07T20:37:01.074517Z",
     "iopub.status.idle": "2022-01-07T20:37:01.089504Z",
     "shell.execute_reply": "2022-01-07T20:37:01.088723Z",
     "shell.execute_reply.started": "2022-01-07T20:37:01.074924Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshape the data for CNN\n",
    "X_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(len(X_test), X_test.shape[1], 1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:37:06.711106Z",
     "iopub.status.busy": "2022-01-07T20:37:06.710847Z",
     "iopub.status.idle": "2022-01-07T20:37:06.72126Z",
     "shell.execute_reply": "2022-01-07T20:37:06.720452Z",
     "shell.execute_reply.started": "2022-01-07T20:37:06.711079Z"
    }
   },
   "outputs": [],
   "source": [
    "# making the deep learning function\n",
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
    "                    padding='same', input_shape=(72, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # adding a pooling layer\n",
    "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
    "                    padding='same', input_shape=(72, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
    "                    padding='same', input_shape=(72, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:37:08.721648Z",
     "iopub.status.busy": "2022-01-07T20:37:08.721262Z",
     "iopub.status.idle": "2022-01-07T20:37:11.175273Z",
     "shell.execute_reply": "2022-01-07T20:37:11.174523Z",
     "shell.execute_reply.started": "2022-01-07T20:37:08.721614Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:49:42.117404Z",
     "iopub.status.busy": "2022-01-07T20:49:42.116943Z",
     "iopub.status.idle": "2022-01-07T20:54:04.096377Z",
     "shell.execute_reply": "2022-01-07T20:54:04.095505Z",
     "shell.execute_reply.started": "2022-01-07T20:49:42.117371Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_accuracy', save_best_only=True)\n",
    "his = model.fit(X_train, y_train, epochs=30, batch_size=32, \n",
    "                validation_data=(X_test, y_test), callbacks=[logger, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results (CNN)\n",
    "Let's make a graphical visualization of results obtained by applying CNN to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:58:37.27726Z",
     "iopub.status.busy": "2022-01-07T20:58:37.277011Z",
     "iopub.status.idle": "2022-01-07T20:58:37.949161Z",
     "shell.execute_reply": "2022-01-07T20:58:37.948458Z",
     "shell.execute_reply.started": "2022-01-07T20:58:37.277233Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the model performance on test data\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:54:59.581063Z",
     "iopub.status.busy": "2022-01-07T20:54:59.580754Z",
     "iopub.status.idle": "2022-01-07T20:54:59.587174Z",
     "shell.execute_reply": "2022-01-07T20:54:59.586306Z",
     "shell.execute_reply.started": "2022-01-07T20:54:59.581025Z"
    }
   },
   "outputs": [],
   "source": [
    "# check history of model\n",
    "history = his.history\n",
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-07T20:55:02.783182Z",
     "iopub.status.busy": "2022-01-07T20:55:02.782629Z",
     "iopub.status.idle": "2022-01-07T20:55:03.423135Z",
     "shell.execute_reply": "2022-01-07T20:55:03.42247Z",
     "shell.execute_reply.started": "2022-01-07T20:55:02.783143Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['loss']) + 1)\n",
    "acc = history['accuracy']\n",
    "loss = history['loss']\n",
    "val_acc = history['val_accuracy']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "# visualize training and val accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Training and Validation Accuracy (CNN)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(epochs, acc, label='accuracy')\n",
    "plt.plot(epochs, val_acc, label='val_acc')\n",
    "plt.legend()\n",
    "\n",
    "# visualize train and val loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Training and Validation Loss(CNN)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(epochs, loss, label='loss', color='g')\n",
    "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion after CNN Training\n",
    "After training our deep CNN model on training data and validating it on validation data, it can be interpreted that:\n",
    "* Model was trained on 50 epochs and then on 30 epochs\n",
    "* CNN performed exceptionally well on training data and the accuracy was **99%**\n",
    "* Model accuracy was down to **83.55%** on valiadtion data after **50** iterations, and gave a good accuracy of **92%** after **30** iterations. Thus, it can be interpreted that optimal number of iterations on which this model can perform are **30**."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 902298,
     "sourceId": 1530359,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 492658,
     "sourceId": 2378330,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30152,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
